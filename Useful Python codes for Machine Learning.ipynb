{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Python codes for Machine Learning\n",
    "This notebook contains a list of scripts to train the machine learning model for my reference.\n",
    "<n>\n",
    "It is based on an udemy course \"Python for Data Science and Machine Learning Bootcamp\" by Jose Portilla <br>\n",
    "https://www.udemy.com/python-for-data-science-and-machine-learning-bootcamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFUL LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "# sklearn\n",
    "# tensorflow\n",
    "# keras\n",
    "\n",
    "# for linear regression, logistics regression, knn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# for spliting data into training and testing model\n",
    "#from sklearn.cross_validation import train_test_split #old version\n",
    "from sklearn.model_selection import train_test_split #version 0.20 onwards\n",
    "\n",
    "# for calculating MAE, MSE, RMSE (for linear regression)\n",
    "from sklearn import metrics\n",
    "\n",
    "# for getting the precision and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# see visualisation in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Useful article\n",
    " https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/\n",
    "\n",
    " In the data exploration, we want to\n",
    " - find the correlation between each column\n",
    " - check if there's any missing data\n",
    " - if there's missing data, see if you can impute(fill) the missing values by getting the other columns with good correlation, then use various methods and substitute the missing values. \n",
    " - Otherwise, drop it\n",
    " - check if there's any outliers\n",
    " - if there's outliers, find out why, and determine if you should drop or not\n",
    " - ** if outlier is incorrectly entered: drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA INFO\n",
    "# df = data\n",
    "# to get the info of the data\n",
    "df.info()\n",
    "\n",
    "# to show data columns\n",
    "df.columns\n",
    "\n",
    "# to show descriptive stats info of each columns (eg: mean, std_dev, min, max etc)\n",
    "df.describe()\n",
    "\n",
    "# to show the correlation of the data (which column has a good relationship with other column)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATIONS\n",
    "\n",
    "# to turn the seaborn plot style to whitegrid\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# PAIRPLOT: to see histograms of all the columns and correlation of scatterplots\n",
    "sns.pairplot(df)\n",
    "\n",
    "# to see histograms of all the columns and correlation, color code in a column\n",
    "sns.pairplot(df, hue='*column_name*')\n",
    "\n",
    "# HEATMAP: to show the correlation in heatmap\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "\n",
    "# HISTOGRAM: to check out the distribution & the frequency of a column\n",
    "# ** bins = range of values divided into xx equal parts\n",
    "# ** to turn off the kde, kde=False\n",
    "sns.distplot(df['*column_name*'], bins=30)\n",
    "\n",
    "# another way to plot distribution\n",
    "df['*column_name*'].plot.hist(bins=30)\n",
    "\n",
    "# to increase the size of the plot, use figsize = ()\n",
    "# plt.figure(figsize=(10,7)) or\n",
    "df['*column_name*'].hist(bins=30, figsize=(10,4))\n",
    "\n",
    "# BOXPLOT: useful for comparing distributions across groups & checkout outliers\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.boxplot(x='*x_column_name*',y='*y_column_name*', data=df)\n",
    "\n",
    "\n",
    "# CHECK MISSING DATA\n",
    "# to chekc missing data, use heatmap to check missing data, yellow strips = null values\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap ='viridis')\n",
    "\n",
    "# to count a feature, use countplot, if adding categorize by another feature, include in hue\n",
    "# eg: count the number of titanic survivor by gender\n",
    "# sns.countplot(x='Survived', hue='Sex', data=df, palette='RdBu_r')\n",
    "sns.countplot(x='*column_name*', hue='*category_column_name*', data=df, palette='*color_name*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for interactive plot, you can use cufflinks\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "# eg: histogram interactive plot\n",
    "df['*column_name*'].iplot(kind='hist', bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "- Used when the dependent variable (the variable you want to predict,y) is numeric\n",
    "- linear approach to modelling the relationship between a scalar response and >= 1 explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOMNINATE X AND Y\n",
    "# X = featured data used to predict y\n",
    "# y = data you want to predict\n",
    "X = df[[]]\n",
    "y = df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training set and testing set of the model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import linear regression model and create it\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AND TEST THE MODEL\n",
    "# train the model \n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# predict the testing sets with the model\n",
    "predict = lm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE THE RESULTS\n",
    "# to check out the preiction model compare to the actual, plot a scatter\n",
    "# if it shows straight line = quite a good model\n",
    "plt.scatter(y_test, predictions)\n",
    "\n",
    "# to check out the residuals, you can plot histogram of the distribution\n",
    "sns.distplot((y_test-predict))\n",
    "\n",
    "# ** RESIDUALS: difference between actual values of y test and the predicted values,\n",
    "# it's a measure of how well a predict line fits an individual actual data point, basically in the distribution plot\n",
    "# we want to get as much 0 as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL RESULTS: COEFFICIENTS\n",
    "# to check out the coefficient of each features\n",
    "lm.coef_\n",
    "\n",
    "# to make the coefficient into a table\n",
    "cdf = pd.DataFrame(lm.coef_, X.columns, columns=['Coeff'])\n",
    "cdf\n",
    "\n",
    "# coefficient tells you that a 1 unit increase in a feature is associated with an increase of a coefficient \n",
    "# of the predicted value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL RESULTS: EVALUATION METRICS\n",
    "# Evaluate metrics, basically: \n",
    "# - Mean Absolute Error (MAE)\n",
    "# - Mean Squared Error (MSE)\n",
    "# - Root Mean Squared Error (RMSE)\n",
    "# You want to minimize all of them, the smaller number the better\n",
    "from sklearn import metrics\n",
    "\n",
    "# Mean absolute error\n",
    "MAE = metrics.mean_absolute_error(y_test, predict)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = metrics.mean_squared_error(y_test, predict)\n",
    "\n",
    "# Root Mean Squred Error (RMSE)\n",
    "RMSE = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistics Regression\n",
    "- used when the dependent variable is categorical (classification problem)\n",
    "- used to predict the odds of being a case based on the values of the independent variables (predictors,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical features using pandas' get_dummies\n",
    "# eg: turned Female & Male to 0 & 1 so that the ML algorithm will understand the categorical features\n",
    "# drop_first = True, you want to drop the 1st column since the other col already categorize it\n",
    "# eg: male column, 0 = female, 1 = male.\n",
    "cat_feat = pd.get_dummies(df['*column_name_to_transform*'], drop_first=True)\n",
    "\n",
    "# concatenate with the data\n",
    "df = pd.concat([cat_feat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOMINATE X AND Y\n",
    "X = df[[]] # only numerical featured values \n",
    "y = df[] # data you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training set and testing set of the model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logistic regression model and create it\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AND TEST THE MODEL\n",
    "# train the model \n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "# predict the testing sets with the model\n",
    "predict = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL RESULTS: EVALUATE THE MODEL\n",
    "# using classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# show classification report\n",
    "print(classification_report(y_test, predict))\n",
    "\n",
    "# show confusion matrix\n",
    "print(confusion_matrix(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix: <br>\n",
    "TN, FP <br>\n",
    "FN, TP <br>\n",
    "TN = true negatives, predict = no, actual = no <br>\n",
    "FP = False positive, predict = yes, actual = no <br>\n",
    "FN = false negative, predict = no, actual = yes <br>\n",
    "TP = true positive, predict =yes, actual = yes <br>\n",
    "<n>\n",
    "You would want to have a very high TN & TP and low FP & FN. \n",
    "\n",
    "<n>\n",
    "As for classification report, you would want a very high precision (close to 1)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbours\n",
    "- Supervised Technique\n",
    "- Used for Classification or Regression of known data where usually the target variable is known before hand\n",
    "- K = number of nearest neighbours used to classify. \n",
    "eg: you have X to group into A and B, k = 3, X nearest neighbour = 2 in group A and 1 in group B, X is group A. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Because the KNN and classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variable actually matters a lot, and any of the variables that have large scale, will have a much larger effect on the distance between observations. Thus when you use KNN you wanna standardized everything in the same scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE THE FEATURED DATA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fits the scaler to the data (finds the min, max, mean that will later use in its transform operation)\n",
    "# remember to drop the column you want to predict\n",
    "df_notc = df.drop('*y_column_name*', axis=1)\n",
    "scaler.fit(df_notc)\n",
    "\n",
    "# use scalar to do transformation\n",
    "scaled_feature = scaler.transform(df_notc)\n",
    "\n",
    "# create a table for scaled feature\n",
    "df_feat = pd.DataFrame(scaled_feature, columns=df_notc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOMINATE X AND Y\n",
    "X = df_feat #standardized featured data\n",
    "y = df[] #data you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training set and testing set of the model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KNN model and create it\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_neighbors = 1 is k=1 <br>\n",
    "When tested with a new example, it looks through the training data and finds the k training examples that are closest to the new example. k thus is the number of neighbors considered. <br>\n",
    "eg: k=2, the 2 closest neighbors are used to smooth the estimate at a given point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AND TEST THE MODEL\n",
    "# train the model \n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict the testing sets with the model\n",
    "predict = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL RESULTS: EVALUATE THE MODEL\n",
    "# using classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# show classification report\n",
    "print(classification_report(y_test, predict))\n",
    "\n",
    "# show confusion matrix\n",
    "print(confusion_matrix(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will show the results with k=1, If the results aren't good, or if you wish to improve more, use elbow method (for loop) to pick the best k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method\n",
    "error_rate[]\n",
    "\n",
    "for i in range(1,40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    \n",
    "    # append the error rate into the list\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error rate\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40), error_rate,color='blue', linestyle='--',marker='o',markersize=10,markerfacecolor='red')\n",
    "plt.title('Error Rate vs K Value')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you plot it, check which k value brings the lowest error rate, you would want to pick a number that has been consistently producing lower error rate before and after. \n",
    "<n>\n",
    "Once you've picked the k value, re-run the model again to check the precision and the confusion matrix to see if you're getting a better result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- both are supervised technique\n",
    "- Used for classification and regression\n",
    "- Decision trees = behaves with \"if this then that\" conditions ultimately yielding a specific result.\n",
    "- DT are easy to interpret, can handle both numerical and categorical data, performs well on large datasets, but are prone to overfitting.\n",
    "- Random Forest = a collection or ensemble of decision trees. A fraction of the number of rows is selected at random and a particular number of features are selected at random to train on and a decision tree is built on this subset. <br>\n",
    "ie: (a collection of trees = Forest, and trees being trained on subsets which are being selected at random)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between decision trees and logistic regression\n",
    "- logistic regression : searching fo a single linear decision boundary in the feature space. You manuallly add interactions terms.\n",
    "- decision trees: partitioning the feature space into half-space using axis-aligned linear decision boundaries. The net effect is that you have a non-linear decision boundary, possibly more than one. Automatically take into account interaction between variables.\n",
    "<p>\n",
    "\n",
    "It's always good to try both models and do cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOMINATE X AND Y\n",
    "X = df[[]] # only numerical featured values \n",
    "y = df[] # data you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training set and testing set of the model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with 1 decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# train with Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# decision tree\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "# random forest, you can work with the estimator numbers, 200 is alright\n",
    "rfc = RandomForestClassifier(n_estimator=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the data with decision tree/Random Forest\n",
    "dtree.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test data with the model\n",
    "prediction = dtree.predict(X_test) # --OR\n",
    "precition = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL RESULTS: EVALUATE THE MODEL\n",
    "# using classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# show classification report\n",
    "print(classification_report(y_test, prediction))\n",
    "\n",
    "# show confusion matrix\n",
    "print(confusion_matrix(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "- Supervised learning\n",
    "- Used for classification or regression problems.\n",
    "- Uses a technique called the kernel trick to transform data and then based on these transformations it finds an optimal boundary between the possible outputs. (does some extremely complex data transformation, then figures out how to separate the data based on the labels or outputs you've defined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest & SVM\n",
    "- SVM performs better on sparse data.\n",
    "- Random Forest is suited for multiclass problems.\n",
    "\n",
    "<p>\n",
    "But it's always to test out all similar models. SVM, Decision Trees/Random Forest, logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOMINATE X AND Y\n",
    "X = df[[]] # only numerical featured values \n",
    "y = df[] # data you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training set and testing set of the model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with svm\n",
    "from sklearn.svm import svc\n",
    "\n",
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test data using the model\n",
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results if the model is good enough. Or you can use grid search to find the best parameters to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search=\n",
    "- C: controls the cost of misclassifictaion on the training data. Large C values = low bias & high variance, low bias because you penalize the cost of misclassification a lot. If small C values, you are not going to penalize that cost as much, so high bias low variance.\n",
    "- Gamma: has to do with gaussian radio base function (rbf), which is what kernel ='rbf' shows. Small gamma=gaussian for large variance. Big gamma=lead to high bias & low variance in the model.\n",
    "<br>\n",
    "if gamma = large, variance = small --> support vector don't have widespread "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH\n",
    "# find the correct parameters to train a model in svm\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary where the keys are the actual parameters that go into the model you're using (SVC)\n",
    "param_grid = {'C':[0.1,1,10,100,1000], 'gamma':[1,0.1,0.01,0.001,0.0001]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid search\n",
    "# verbose = the higher the number, the more verbose (text output of the description of the process)\n",
    "grid = GridSearchCV(SVC(), param_grid, verbose=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** NOTE: grid search could take a long time depending how large the data is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameters of the grid\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best estimator for the SVC\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test data with the best parameters model\n",
    "grid_predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL RESULTS: EVALUATE THE MODEL\n",
    "# using classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# show classification report\n",
    "print(classification_report(y_test, grid_prediction))\n",
    "\n",
    "# show confusion matrix\n",
    "print(confusion_matrix(y_test, grid_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With grid search to find the best parameters, usually it performs better (higher accuracy) compare to the one without grid search. The larger the data the more visible it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clusters\n",
    "- Unsupervised learning\n",
    "- Used in unlabeled data (i.e., data without defined categories or groups)\n",
    "- Solved clustering problem by classifying a given data set into a number of clusters (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use K-means cluster\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nominate number of clusters\n",
    "kmeans = KMeans(n_clusters=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** NOTE: There's no easy answer for choosing the best k value, but one way to try and do it is by __Elbow method__. \n",
    "<p>\n",
    "\n",
    "We use that method in __K Nearest Neighbors__\n",
    "\n",
    "<p>\n",
    "\n",
    "1. Compute the sum of squared error (SSE) for some values of k (eg:2,4,6,8...). The SSE is defined as the sum of the squared distance between each member of the cluster and its centroid.\n",
    "2. When plot k against the SSE, you'll see the error decreases as k gets larger. When cluster# increases, they should be smaller so distortion is also smaller.\n",
    "3. Elbow method = choose the k at which the SSE decreases abruptly, meaning you want to choose the k where you don't get much information by increasing the cluster# aka not going to significantly decrease within the groups SSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "kmeans.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the centroids of the clusters\n",
    "kmeans.clusters_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the predicted clusters for the data\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot and see how the model predicts the clustering\n",
    "plt.scatter(x1_data, x2_data, c=kmeans.labels_, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "- unsupervised learning\n",
    "- statistical procedure that uses orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "- often used to make data easy to explore and visualize\n",
    "- not used to predict\n",
    "- mostly trying to find out what components are the most important ones that explain the most variance of the data set.- \n",
    "- see here for more info: http://setosa.io/ev/principal-component-analysis/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the PCA data, we need to find the first 2 principal components and visualize the data\n",
    "<p>\n",
    "We need to scale the data so that each feature has a single unit variance before we can use PCA on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data in dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE THE DATA\n",
    "# import standard scaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the dataframe\n",
    "scaler.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform it\n",
    "scaled_data = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# nominate the number of components you want to keep\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the data\n",
    "pca.fit(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "x_pca = pca.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the rows & columns after running the pca\n",
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should just get 2 principal components. We have transform 10++ dimensions (featured data) to just 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "# to visualize 1st and 2nd principal component, c = color\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x_pca[:,0], x_pca[:,1], c= target_data, cmap='plasma')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visualizing, we can easily separate these 2 classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize what principal component represents, we can use heatmap\n",
    "df_comp = pd.DataFame(pca.components_, columns=featured_column_name)\n",
    "\n",
    "# heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df_comp, cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap will show the relationship between the correlation of the various feature and the principal component themselves.\n",
    "<p>\n",
    "The higher the color (towards yellow in plasma), the more correlated to a specific feature in the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  NLP = application of computational techniques to the analysis and synthesis of natural language and speech\n",
    "- helps computer to understand human language (it breaks down and process language)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing the Vectorization (turning each message into a vector that machine learning models can understand), we need to do text-preprocessing - remove punctuation & stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PRE-PROCESSING\n",
    "# Need to remove punctuation (.,@#$%^&; etc) & stop words (I, you, we, she, he, himself, herself, them, themselves etc)\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# data in dataFrame: col: label=spam or ham(normal message), message=sms messages content\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove punctuation and stop words for all messages\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # check characters to see if they're in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    \n",
    "    # join the characters again to form the string\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function\n",
    "messages['message'].apply(text_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When text-preprocessing is done, we need to convert the list of words to an actual vector that scikit-learn can use.The process is called vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 3 steps: <br>\n",
    "1. Count how many times does a word occur in each message (term frequency)\n",
    "2. Weight the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "    \n",
    "<p>\n",
    " Once the vectorization process is done aka text messages have converted to numerical vectors, we can train the model to predict the label.\n",
    "\n",
    "<p>\n",
    " Fortunately, sckit-learn pipeline does all these steps from vectorization to training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZATION & TRAINING\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# use Naive Bayes to train the model\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training set and testing set of the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "msg_train, msg_test, label_train, label_test = train_test_split(messages['message'],messages['label'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "pipeline.fit(msg_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test data with the model\n",
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL RESULTS: EVALUATE THE MODEL\n",
    "# using classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# show classification report\n",
    "print(classification_report(label_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** To use different model from Naive Bayes, eg: Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# pipeline\n",
    "pipelineRF = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)), \n",
    "    ('tfidf', TfidfTransformer()), \n",
    "    ('classifier', RandomForestClassifier()),  \n",
    "])\n",
    "\n",
    "# train the data with Random Forest\n",
    "pipelineRF.fit(msg_train, label_train)\n",
    "\n",
    "# predict the test data with the model\n",
    "predictionsRF = pipelineRF.predict(msg_test)\n",
    "\n",
    "# show result\n",
    "print(classification_report(label_test, predictionsRF))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
