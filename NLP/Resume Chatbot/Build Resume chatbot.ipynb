{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436503bd",
   "metadata": {},
   "source": [
    "# Resume Chatbot\n",
    "This notebook uses Natural Language Processing (NLP) and Neural Network to build a chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c55d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import random \n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e41a97",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume data json file name\n",
    "DATA_FN = 'data/resume_data.json'\n",
    "\n",
    "# parameters for model training\n",
    "EPOCH_NUM = 200\n",
    "LEARNING_RATE = 0.01\n",
    "LOSS_TYPE = 'categorical_crossentropy'\n",
    "S_METRICS = 'accuracy'\n",
    "DECAY = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e51312",
   "metadata": {},
   "source": [
    "### Extract data\n",
    "- Extract resume data from json file\n",
    "- Download words and vocabs from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b7c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract words and vocabs from nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from json file\n",
    "with open(DATA_FN) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data = data['resume']\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing lemmatizer to get stem of words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Each list to create\n",
    "words = []\n",
    "classes = []\n",
    "doc_X = []\n",
    "doc_y = []\n",
    "\n",
    "# Loop through all the intents\n",
    "# tokenize each pattern and append tokens to words, the patterns and\n",
    "# the associated tag to their associated list\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        words.extend(tokens)\n",
    "        doc_X.append(pattern)\n",
    "        doc_y.append(intent['tag'])\n",
    "    \n",
    "    # add the tag to the classes if it's not there already \n",
    "    if intent['tag'] not in classes:\n",
    "        classes.append(intent['tag'])\n",
    "        \n",
    "# lemmatize all the words in the vocab and convert lowercase\n",
    "# if the words don't appear in punctuation\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "\n",
    "# sorting the vocab and classes in alphabetical order and taking the # set to ensure no duplicates occur\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d41f3",
   "metadata": {},
   "source": [
    "### Set up training data\n",
    "Set up data to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cff921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for training data\n",
    "training = []\n",
    "out_empty = [0] * len(classes)\n",
    "\n",
    "# create bag of words model\n",
    "for idx, doc in enumerate(doc_X):\n",
    "    bow = []\n",
    "    text = lemmatizer.lemmatize(doc.lower())\n",
    "    for word in words:\n",
    "        bow.append(1) if word in text else bow.append(0)\n",
    "        \n",
    "    # create index of class that the current pattern is linked to\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes.index(doc_y[idx])] = 1\n",
    "    \n",
    "    # add one hot encoded BoW and its classes to training data \n",
    "    training.append([bow, output_row])\n",
    "    \n",
    "# shuffle the data and convert it to an array\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "\n",
    "# split the features and target labels\n",
    "train_x = np.array(list(training[:, 0]))\n",
    "train_y = np.array(list(training[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a70eb1",
   "metadata": {},
   "source": [
    "### Model training\n",
    "Train the model using neural network. The model will look at the features and predict the tag associated with the features then will select the best response from the given tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# get the length of input and output\n",
    "input_shape = (len(train_x[0]),)\n",
    "output_shape = len(train_y[0])\n",
    "\n",
    "\n",
    "# Run the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=input_shape, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(output_shape, activation = 'softmax'))\n",
    "\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate= LEARNING_RATE, decay=DECAY)\n",
    "model.compile(loss=LOSS_TYPE,\n",
    "              optimizer=adam_opt,\n",
    "              metrics=[S_METRICS])\n",
    "\n",
    "# train the model\n",
    "model.fit(x=train_x, y=train_y, epochs=EPOCH_NUM, verbose=1, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45720ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, acc = model.evaluate(train_x, train_y, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58872fc0",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "Save the model so it can be used in other script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9bdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/resume_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5ec5a",
   "metadata": {},
   "source": [
    "### Test chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f626082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "#model = tf.keras.models.load_model('model/resume_model')\n",
    "\n",
    "# Show the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5f60e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the chatbot\n",
    "try:\n",
    "    while True:\n",
    "        message = input('')\n",
    "        intents = pred_class(message, words, classes, model)\n",
    "        result = get_response(intents, data)\n",
    "        print(result)\n",
    "except (EOFError, KeyboardInterrupt):\n",
    "    print(\"Chat Ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1a978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
